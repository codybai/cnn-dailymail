{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "make_data_file_colab.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/taoyafan/cnn-dailymail/blob/master/make_data_file_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "W5IXhmVWC5Hp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "教程参考：\n",
        "\n",
        "[想免费用谷歌资源训练神经网络？Colab 详细使用教程](https://jinkey.ai/post/tech/xiang-mian-fei-yong-gu-ge-zi-yuan-xun-lian-shen-jing-wang-luo-colab-xiang-xi-shi-yong-jiao-cheng#toc_2)\n",
        "\n",
        "[Google Colab Free GPU Tutorial](https://medium.com/deep-learning-turkey/google-colab-free-gpu-tutorial-e113627b9f5d)"
      ]
    },
    {
      "metadata": {
        "id": "VPP8_4eRsAEy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 重启虚拟机\n",
        "\n",
        "虚拟机上所有的东西都会消失"
      ]
    },
    {
      "metadata": {
        "id": "MjbOuQKdaBUV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !kill -9 -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8CdT0VtJenQu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 授权登录、挂载云盘"
      ]
    },
    {
      "metadata": {
        "id": "mYV4RCpYHUW0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f10cb4e9-cfd7-403f-df3c-dbbef2d8435c"
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "# auth.authenticate_user()\n",
        "# gauth = GoogleAuth()\n",
        "# gauth.credentials = GoogleCredentials.get_application_default()\n",
        "# drive = GoogleDrive(gauth)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "V_wA03CoFuWq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 将数据从云盘读入虚拟机根目录"
      ]
    },
    {
      "metadata": {
        "id": "uPb9bXOmKKtT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 文件名和路径设置"
      ]
    },
    {
      "metadata": {
        "id": "YCdKS0N05TiQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "是否自动生成测试文件？如果是，则从训练集中抽出一部分作为测试集，否则手动设定测试集"
      ]
    },
    {
      "metadata": {
        "id": "0GennuT75Ovd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "AUTO_TEST = True\n",
        "\n",
        "if AUTO_TEST:\n",
        "    TEST_NUM = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LI4cgbiVFg2Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ef01e261-c2c5-4a12-e9e9-1e6727bf0173"
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "stories_dir = \"\"\n",
        "split_stories_dir = \"split_stories\"\n",
        "tokenized_stories_dir = \"tokenized_stories\"\n",
        "finished_files_dir = \"finished_files\"\n",
        "drive_finished_files_dir = \"drive/My Drive/code/get_to_the_point/finished_files\"\n",
        "smrz_dir = \"drive/My Drive/code/get_to_the_point/pointer-generator-master\"\n",
        "chunks_dir = os.path.join(finished_files_dir, \"chunked\")\n",
        "drive_chunks_dir = os.path.join(drive_finished_files_dir, \"chunked\")\n",
        "\n",
        "\n",
        "# file_names = [\"bytecup.corpus.train.{}.txt\".format(i) for i in range(8,9)]\n",
        "train_file_names = [\"bytecup.corpus.train.0.txt\"]\n",
        "val_file_names = [\"bytecup.corpus.validation_set.txt\"]\n",
        "if not AUTO_TEST:\n",
        "    test_file_names = [\"for_test.txt\"]\n",
        "\n",
        "# if not os.path.exists(stories_dir): raise Exception(\"stories_dir %s doesn't exist.\" % (stories_dir))\n",
        "if not os.path.exists(split_stories_dir): os.makedirs(split_stories_dir)\n",
        "if not os.path.exists(tokenized_stories_dir): os.makedirs(tokenized_stories_dir)\n",
        "if not os.path.exists(finished_files_dir): os.makedirs(finished_files_dir)\n",
        "if not os.path.exists(drive_finished_files_dir): os.makedirs(drive_finished_files_dir)\n",
        "if not os.path.exists(chunks_dir): os.makedirs(chunks_dir)\n",
        "# if not os.path.exists(drive_chunks_dir): os.makedirs(drive_chunks_dir)\n",
        "if not os.path.exists(smrz_dir): raise Exception(\"smrz_dir %s doesn't exist.\" % (smrz_dir))\n",
        "    \n",
        "print(\"train_file_names: %s\"% train_file_names)\n",
        "print(\"val_file_names: %s\"% val_file_names)\n",
        "if not AUTO_TEST:\n",
        "    print(\"test_file_names: %s\"% test_file_names)\n",
        "else:\n",
        "    print(\"test_file_names: %s\"% train_file_names)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_file_names: ['bytecup.corpus.train.0.txt']\n",
            "val_file_names: ['bytecup.corpus.validation_set.txt']\n",
            "test_file_names: ['bytecup.corpus.train.0.txt']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uN7X2j22fHyP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 复制数据原始文件（file_names）"
      ]
    },
    {
      "metadata": {
        "id": "yEanA2zaRWAr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "28dea17e-6de4-49eb-8b8f-73d9730d8a92"
      },
      "cell_type": "code",
      "source": [
        "drive_dir = \"drive/My Drive/code\"\n",
        "\n",
        "def copy_file(file_names, drive_dir):\n",
        "    for file_name in file_names:\n",
        "        src_file = os.path.join(drive_dir, file_name)\n",
        "        print(\"Copy file from \\\"%s\\\" to \\\"%s\\\"\"% (src_file, file_name))\n",
        "        shutil.copyfile(src_file, file_name) \n",
        "\n",
        "copy_file(train_file_names, drive_dir)\n",
        "copy_file(val_file_names, drive_dir)\n",
        "if not AUTO_TEST:\n",
        "    copy_file(test_file_names, drive_dir)\n"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copy file from \"drive/My Drive/code/bytecup.corpus.train.0.txt\" to \"bytecup.corpus.train.0.txt\"\n",
            "Copy file from \"drive/My Drive/code/bytecup.corpus.validation_set.txt\" to \"bytecup.corpus.validation_set.txt\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VqfFntKvWeRg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 原始方法，不需要授权登录"
      ]
    },
    {
      "metadata": {
        "id": "gZOKL_F4KpIA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "显示云盘根目录的文件"
      ]
    },
    {
      "metadata": {
        "id": "1ckcHIzHMvaT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# file_list = drive.ListFile({'q': \"'root' in parents and trashed=false\"}).GetList()\n",
        "# for file1 in file_list:\n",
        "#       print('title: %s, id: %s, mimeType: %s' % (file1['title'], file1['id'], file1[\"mimeType\"]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NYHBWvYwfVDu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "打开 code 文件"
      ]
    },
    {
      "metadata": {
        "id": "qg5hCNcWNTik",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# file_list = drive.ListFile({'q': \"'18ijhbBiNtaqpgiNwzxAf6a3W13Pc8qSa' in parents and trashed=false\"}).GetList()\n",
        "# for file1 in file_list:\n",
        "#       print('title: %s, id: %s, mimeType: %s' % (file1['title'], file1['id'], file1[\"mimeType\"]))\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tyBzeAQxffhF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "读取云盘中的文件内容"
      ]
    },
    {
      "metadata": {
        "id": "wV0Sz8WlO0F8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_file_content(file_name, show = False):\n",
        "    id = [file['id'] for file in file_list if file['title'] == file_name][0]\n",
        "  \n",
        "    if id == []:\n",
        "        print(\"There are not a file named %s\"% file_name)\n",
        "        return []\n",
        "  \n",
        "    print(\"Matched file named %s, and the id is %s\"% (file_name, id))\n",
        "    file = drive.CreateFile({'id': id})\n",
        "    txt = file.GetContentString()\n",
        "    if show:\n",
        "        print(\"This is the content of file %s :\\n\\n\"% file_name)\n",
        "        print(txt)\n",
        "    \n",
        "    print(\"Read file complete, length of file is %d\" % len(txt))\n",
        "    return txt\n",
        "  \n",
        "# txt = get_file_content(file_name, show = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q-GysjSBKjQW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "内容写入虚拟机"
      ]
    },
    {
      "metadata": {
        "id": "DrqNX9tVlpOT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# with open(file_name, 'w') as f:\n",
        "#     f.write(txt)\n",
        "# !ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XRe7Yv8wDNcZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "读取根目录文件，测试写入成功"
      ]
    },
    {
      "metadata": {
        "id": "Xdibi1_9l8On",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def show_file(str):\n",
        "    with open(str, 'r') as load_f:\n",
        "        print(\"This is the content of file %s :\\n\\n\"% str)\n",
        "        lines = load_f.readlines()\n",
        "        for i, line in enumerate(lines):\n",
        "            print(line)\n",
        "        print(\"\\n\\n\")\n",
        "    \n",
        "# show_file(file_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Yw4dPrz5DTDs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "删除文件，例如\n",
        "\n",
        "remove_files(os.listdir(dir))\n",
        "\n",
        "可以删除路径dir下所有文件"
      ]
    },
    {
      "metadata": {
        "id": "_J0xxyoGmEn6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def remove_files(file_names):\n",
        "    for file_name in file_names:\n",
        "        os.remove(file_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LMh8LdXL9sLT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 开始对数据进行预处理"
      ]
    },
    {
      "metadata": {
        "id": "tDQtyZ_hKFdH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 将每个数据集文件拆分个多个小文件\n",
        "每个小文件包含一个故事的内容和标题，以 ***数据类型_ ID***  命名，如 ***train_1*** 存放在 split_stories_dir 下"
      ]
    },
    {
      "metadata": {
        "id": "P0-C9YAaGldG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import re\n",
        "import random\n",
        "\n",
        "def split_lines(lines, story_type):\n",
        "    '''\n",
        "        输入参数 lines 是多行的数据文件，每行是一个样本，样本类型为 story_type\n",
        "        \n",
        "        将输入的 lines 按行拆分，分别写入文件，文件名为 story_type_id\n",
        "    '''\n",
        "    print('Writting %s file in %s...'% (story_type, split_stories_dir))\n",
        "    \n",
        "    # 读取数据集的每一个样本\n",
        "    for i, line in enumerate(lines):\n",
        "        txt_dic = json.loads(line)\n",
        "        split_file_dir = os.path.join(split_stories_dir, \"%s_%s\"%(story_type, txt_dic['id']))\n",
        "\n",
        "        # 将每个样本单独写入一个文件中\n",
        "        with open(split_file_dir, \"w\") as f:\n",
        "            txt_dic['content'] = re.sub(r\"(?<=[a-zA-Z\\\"\\'\\[\\]\\(\\)\\-\\$]{2})[.](?=\\S)|[.](?=[a-zA-Z\\\"\\'\\[\\]\\(\\)\\-\\$]{2})\",\n",
        "                                        \". \", txt_dic['content']) \n",
        "            f.write(txt_dic['content'])\n",
        "            f.write(' . ')\n",
        "            if('title' in txt_dic.keys()):\n",
        "                f.write(\"\\n\\n@title.\\n\")\n",
        "                f.write(txt_dic['title'])\n",
        "\n",
        "\n",
        "def split_story_files(file_names, file_type):\n",
        "    for name in file_names:\n",
        "        file_dir = os.path.join(stories_dir, name)\n",
        "        heads = []\n",
        "        desc = []\n",
        "\n",
        "        print('Opening file: %s...' % name)\n",
        "        time_start = time.time()\n",
        "\n",
        "        # 打开每一个数据集\n",
        "        with open(file_dir, 'r') as load_f:\n",
        "            lines = load_f.readlines()\n",
        "        \n",
        "        if AUTO_TEST and file_type == 'train':\n",
        "#             random.shuffle(lines)\n",
        "            split_lines(lines[: TEST_NUM], 'test')\n",
        "            split_lines(lines[TEST_NUM : -1], 'train')\n",
        "        else:\n",
        "            split_lines(lines, file_type)\n",
        "\n",
        "        print(\"completed\")\n",
        "        time_end = time.time()\n",
        "        print('Consume time:', time_end - time_start, '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K_Tyirt28ZAf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "73781940-9acf-4459-e49c-0e69cac7b545"
      },
      "cell_type": "code",
      "source": [
        "# shutil.rmtree(split_stories_dir)\n",
        "# os.makedirs(split_stories_dir)\n",
        "\n",
        "split_story_files(train_file_names, \"train\")\n",
        "split_story_files(val_file_names, \"val\")\n",
        "\n",
        "if not AUTO_TEST:\n",
        "    split_story_files(test_file_names, \"test\")\n",
        "    \n"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Opening file: bytecup.corpus.train.0.txt...\n",
            "Writting test file in split_stories...\n",
            "Writting train file in split_stories...\n",
            "completed\n",
            "Consume time: 59.97026228904724 \n",
            "\n",
            "Opening file: bytecup.corpus.validation_set.txt...\n",
            "Writting val file in split_stories...\n",
            "completed\n",
            "Consume time: 0.30092930793762207 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TqMYtPjxLBl1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 对每篇文章进行分词（分句）"
      ]
    },
    {
      "metadata": {
        "id": "aja5rYQoLSng",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 安装 java 添加环境变量"
      ]
    },
    {
      "metadata": {
        "id": "gtJ5rgf4Gq-S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "869d5032-ec70-4b0d-8206-a1376e843637"
      },
      "cell_type": "code",
      "source": [
        "time_start = time.time()\n",
        "\n",
        "!apt-get install openjdk-8-jdk\n",
        "!java -version\n",
        "os.environ['CLASSPATH']=\"/content/drive/My Drive/pack/stanford-corenlp-full-2018-10-05/stanford-corenlp-3.9.2.jar\"\n",
        "\n",
        "time_end = time.time()\n",
        "print('Consume time:', time_end - time_start)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "openjdk-8-jdk is already the newest version (8u181-b13-1ubuntu0.18.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 5 not upgraded.\n",
            "openjdk version \"1.8.0_181\"\n",
            "OpenJDK Runtime Environment (build 1.8.0_181-8u181-b13-1ubuntu0.18.04.1-b13)\n",
            "OpenJDK 64-Bit Server VM (build 25.181-b13, mixed mode)\n",
            "Consume time: 4.4622344970703125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OiH4rB0VLXLo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 选择是否分句\n",
        "若为 DIVIDE_SENTENCE = True 则分句加分词，否则只分词"
      ]
    },
    {
      "metadata": {
        "id": "JA3wGJvJLXtg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "DIVIDE_SENTENCE = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zTn1OF01LqOx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 只分词不分句\n",
        "先生成映射文件 mapping，再使用 PTBTokenizer 对文件进行直接操作"
      ]
    },
    {
      "metadata": {
        "id": "J-faSaLQHrwf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "582695c4-2226-429e-9c4e-4fe3597e0433"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import subprocess\n",
        "\n",
        "if not DIVIDE_SENTENCE:\n",
        "\n",
        "    time_start = time.time()\n",
        "    \n",
        "    stories = os.listdir(split_stories_dir)\n",
        "    #生成映射文件 mapping.txt\n",
        "    print(\"Making list of files to tokenize...\")\n",
        "    with open(\"mapping.txt\", \"w\") as f:\n",
        "        for s in stories:\n",
        "            f.write(\"%s \\t %s\\n\" % (os.path.join(split_stories_dir, s), os.path.join(tokenized_stories_dir, s)))\n",
        "    print(\"completed\\n\")\n",
        "    \n",
        "    print(\"Token file frome \\\"%s\\\" to \\\"%s\\\"\"% (split_stories_dir, tokenized_stories_dir))\n",
        "    # Doesn't work\n",
        "    # command = ['java', 'edu.stanford.nlp.process.DocumentPreprocessor', '-ioFileList', '-preserveLines', 'mapping.txt']\n",
        "    \n",
        "    command = ['java', 'edu.stanford.nlp.process.PTBTokenizer', '-ioFileList', '-preserveLines', 'mapping.txt']\n",
        "    subprocess.call(command)\n",
        "    \n",
        "    # Doesn't work\n",
        "    # os.system(\"java edu.stanford.nlp.process.DocumentPreprocessor -ioFileList -preserveLines mapping.txt\")\n",
        "    \n",
        "    time_end = time.time()\n",
        "    print('Consume time:', time_end - time_start)\n",
        "    os.remove(\"mapping.txt\")"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Making list of files to tokenize...\n",
            "completed\n",
            "\n",
            "Token file frome \"split_stories\" to \"tokenized_stories\"\n",
            "Consume time: 61.52289152145386\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gfbXupXgLtXs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 分句加分词\n",
        "无法利用映射文件进行分词，有BUG，故只能一个一个文件分词"
      ]
    },
    {
      "metadata": {
        "id": "XQ5n9tWaHuGP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "if DIVIDE_SENTENCE:\n",
        "    \n",
        "    time_start = time.time()\n",
        "    \n",
        "    stories = os.listdir(split_stories_dir)\n",
        "    for s in stories:\n",
        "        os.system(\"java edu.stanford.nlp.process.DocumentPreprocessor -preserveLines < %s > %s\" %\n",
        "                  (os.path.join(split_stories_dir, s), os.path.join(tokenized_stories_dir, s)))\n",
        "\n",
        "    time_end = time.time()\n",
        "    print('Consume time:', time_end - time_start)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eKNkn9ecMLM6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 格式化文章并存储"
      ]
    },
    {
      "metadata": {
        "id": "Dx3N1rbkMd_0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### （函数）根据文件地址读文件，返回行列表"
      ]
    },
    {
      "metadata": {
        "id": "FIpCbhHFMhQ-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def read_text_file(text_file):\n",
        "    lines = []\n",
        "    with open(text_file, \"r\") as f:\n",
        "        for line in f:\n",
        "            lines.append(line.strip())\n",
        "        return lines"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RsazT-JuMh3R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### （函数）根据文件内容获得文章和摘要"
      ]
    },
    {
      "metadata": {
        "id": "QgR0ehodMkEf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "SENTENCE_START = '<s>'\n",
        "SENTENCE_END = '</s>'\n",
        "\n",
        "def get_art_abs(story_file):\n",
        "    lines = read_text_file(story_file)\n",
        "\n",
        "    # Lowercase everything\n",
        "    lines = [line.lower() for line in lines]\n",
        "\n",
        "    # Separate out article and abstract sentences\n",
        "    article_lines = []\n",
        "    titles = []\n",
        "    next_is_title = False\n",
        "    for idx,line in enumerate(lines):\n",
        "        if line == \"\":\n",
        "            continue # empty line\n",
        "        elif line.startswith(\"@title\"):\n",
        "            next_is_title = True\n",
        "        elif next_is_title:\n",
        "            titles.append(line)\n",
        "        else:\n",
        "            article_lines.append(line)\n",
        "\n",
        "    # Make article into a single string\n",
        "    article = ' '.join(article_lines)\n",
        "\n",
        "    # Make abstract into a signle string, putting <s> and </s> tags around the sentences\n",
        "    title = ' '.join([\"%s %s %s\" % (SENTENCE_START, sent, SENTENCE_END) for sent in titles])\n",
        "\n",
        "    return article, title"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xbkjG56eMkuE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 写入 bin 文件"
      ]
    },
    {
      "metadata": {
        "id": "AmvFRh4jMnB5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.core.example import example_pb2\n",
        "import struct\n",
        "import glob\n",
        "\n",
        "def write_to_bin(src_dir, out_dir, file_type):\n",
        "\n",
        "    time_start = time.time()\n",
        "\n",
        "    out_file = os.path.join(out_dir, '%s.bin'% file_type)\n",
        "    print(\"Writing file %s.bin to %s...\"% (file_type, out_dir) )\n",
        "    with open(out_file, 'wb') as writer:\n",
        "        stories = glob.glob(os.path.join(tokenized_stories_dir, '%s_*'% file_type))\n",
        "#         stories = os.listdir(src_dir)\n",
        "#         print(stories)\n",
        "        for idx, s in enumerate(stories):\n",
        "#             story_file = os.path.join(tokenized_stories_dir, s)\n",
        "            article, title = get_art_abs(s)\n",
        "#             print(\"article: \\n%s\\n\\n\"%article)\n",
        "#             print(\"title: \\n%s\\n\" % title)\n",
        "\n",
        "            tf_example = example_pb2.Example()\n",
        "            tf_example.features.feature['article'].bytes_list.value.extend([article.encode()])\n",
        "            tf_example.features.feature['abstract'].bytes_list.value.extend([title.encode()])\n",
        "            tf_example_str = tf_example.SerializeToString()\n",
        "\n",
        "            str_len = len(tf_example_str)\n",
        "            writer.write(struct.pack('q', str_len))\n",
        "            writer.write(struct.pack('%ds' % str_len, tf_example_str))\n",
        "\n",
        "    time_end = time.time()\n",
        "    print('Consume time:', time_end - time_start)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IU29liCkF24n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "f91665f8-a914-45a4-fd5a-66e3c77878e4"
      },
      "cell_type": "code",
      "source": [
        "write_to_bin(tokenized_stories_dir, finished_files_dir, 'train')\n",
        "write_to_bin(tokenized_stories_dir, finished_files_dir, 'test')\n",
        "write_to_bin(tokenized_stories_dir, finished_files_dir, 'val')"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing file train.bin to finished_files...\n",
            "Consume time: 23.602994918823242\n",
            "Writing file test.bin to finished_files...\n",
            "Consume time: 0.22115588188171387\n",
            "Writing file val.bin to finished_files...\n",
            "Consume time: 0.37483882904052734\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3UlhUOsGo68w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# stories = os.listdir(tokenized_stories_dir)\n",
        "# remove_files(os.listdir(tokenized_stories_dir))\n",
        "# stories = os.listdir(tokenized_stories_dir)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SQt_-t2UMn6P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 将文件分块，每个块的文章个数为 CHUNK_SIZE"
      ]
    },
    {
      "metadata": {
        "id": "0fENII-cMqgb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "b46cd0bf-d8cc-4f8e-a1cd-2c7c93f9e4fe"
      },
      "cell_type": "code",
      "source": [
        "CHUNK_SIZE = 5000 # num examples per chunk, for the chunked data 1000\n",
        "\n",
        "def chunk_file(set_name):\n",
        "        \n",
        "    in_file = os.path.join(finished_files_dir, '%s.bin' % set_name)\n",
        "    \n",
        "    if not os.path.exists(in_file):\n",
        "        print(\"File  %s not exist\"% in_file)\n",
        "        return\n",
        "    \n",
        "    with open(in_file, 'rb') as reader:\n",
        "        chunk = 0\n",
        "        finished = False\n",
        "        while not finished:\n",
        "            chunk_fname = os.path.join(chunks_dir, '%s_%03d.bin' % (set_name, chunk)) # new chunk\n",
        "            with open(chunk_fname, 'wb') as writer:\n",
        "                for _ in range(CHUNK_SIZE):\n",
        "                    len_bytes = reader.read(8)\n",
        "                    if not len_bytes:\n",
        "                        finished = True\n",
        "                        break\n",
        "                    str_len = struct.unpack('q', len_bytes)[0]\n",
        "                    example_str = struct.unpack('%ds' % str_len, reader.read(str_len))[0]\n",
        "                    writer.write(struct.pack('q', str_len))\n",
        "                    writer.write(struct.pack('%ds' % str_len, example_str))\n",
        "                chunk += 1\n",
        "            \n",
        "def chunk_all():\n",
        "    # Chunk the data\n",
        "    for set_name in ['train', 'val', 'test']:\n",
        "#     for set_name in ['test']:\n",
        "        print(\"Splitting %s data into chunks...\" % set_name)\n",
        "        chunk_file(set_name)\n",
        "    print(\"Saved chunked data in %s\" % chunks_dir)\n",
        "    \n",
        "# 分块\n",
        "chunk_all()"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Splitting train data into chunks...\n",
            "Splitting val data into chunks...\n",
            "Splitting test data into chunks...\n",
            "Saved chunked data in finished_files/chunked\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "D0bVyPhjFlD0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 替换虚拟机的文件到云盘中\n",
        "\n",
        "替换 chunk 文件夹中的文件"
      ]
    },
    {
      "metadata": {
        "id": "IoLPPzpqFjjP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "c39bb60f-a322-4581-f592-96ec2780269c"
      },
      "cell_type": "code",
      "source": [
        "def copy_files(src_dir, des_dir):\n",
        "    names = os.listdir(src_dir)\n",
        "    num = 0\n",
        "    for name in names:\n",
        "        src_file = os.path.join(src_dir, name)\n",
        "        des_file = os.path.join(des_dir, name)\n",
        "        shutil.copyfile(src_file, des_file) \n",
        "        num += 1\n",
        "    \n",
        "    print(\"Copied %d files from \\n%s \\nto \\n%s\"% (num, src_dir, des_dir))\n",
        "    \n",
        "if drive_chunks_dir != chunks_dir :\n",
        "    if os.path.exists(drive_chunks_dir): \n",
        "        shutil.rmtree(drive_chunks_dir)\n",
        "        \n",
        "    os.makedirs(drive_chunks_dir)\n",
        "    copy_files(chunks_dir, drive_chunks_dir)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copied 32 files from \n",
            "finished_files/chunked \n",
            "to \n",
            "drive/My Drive/code/get_to_the_point/finished_files/chunked\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GGRjTYJQui0t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 运行预测程序"
      ]
    },
    {
      "metadata": {
        "id": "4edzBn7NyP0D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "b39dda08-8155-467f-8d8f-b6459c13caec"
      },
      "cell_type": "code",
      "source": [
        "!pip install pyrouge"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyrouge\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/85/e522dd6b36880ca19dcf7f262b22365748f56edc6f455e7b6a37d0382c32/pyrouge-0.1.3.tar.gz (60kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 2.6MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyrouge\n",
            "  Running setup.py bdist_wheel for pyrouge ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/75/d3/0c/e5b04e15b6b87c42e980de3931d2686e14d36e045058983599\n",
            "Successfully built pyrouge\n",
            "Installing collected packages: pyrouge\n",
            "Successfully installed pyrouge-0.1.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "S9eyI7Wl9AdC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## （函数）执行 shell 命令\n",
        "\n",
        "输入和在 shell 相同的命令，如路径中的空格前需要加\"\\\""
      ]
    },
    {
      "metadata": {
        "id": "RApNywuM9GI6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "\n",
        "def shell_command(str):\n",
        "    \n",
        "    print(str)\n",
        "    try:\n",
        "        out_bytes = subprocess.check_output(str, shell=True, stderr=subprocess.STDOUT)\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        out_bytes = e.output       # Output generated before error\n",
        "        code      = e.returncode   # Return code\n",
        "\n",
        "    out_text = out_bytes.decode('utf-8')\n",
        "    print(out_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Kp7KBWJM-JiY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 设置模式、路径、名称等参数"
      ]
    },
    {
      "metadata": {
        "id": "jeR6uSyr-JLI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "f0178b83-2c39-4ce4-8d41-f7f8fa8feeed"
      },
      "cell_type": "code",
      "source": [
        "# mode = \"decode\"\n",
        "mode = \"train\"\n",
        "data_path = \"/content/finished_files/chunked/train_*\"\n",
        "vocab_path = \"/content/drive/My Drive/code/get_to_the_point/finished_files/vocab\"\n",
        "log_root = \"/content/log\"\n",
        "drive_log_root = \"/content/drive/My Drive/code/get_to_the_point/log\"\n",
        "exp_name = \"data0_train\"\n",
        "exp_dir = os.path.join(log_root, exp_name)\n",
        "drive_exp_dir = os.path.join(drive_log_root, exp_name)\n",
        "single_pass = \"True\"\n",
        "batch_size = \"256\"\n",
        "summarization_file = os.path.join(smrz_dir, \"run_summarization.py\")\n",
        "\n",
        "drive_train_dir = \"/content/drive/My Drive/code/get_to_the_point/log/train\"\n",
        "\n",
        "print(mode)\n",
        "print(data_path)\n",
        "print(vocab_path)\n",
        "print(log_root)\n",
        "print(exp_name)\n",
        "print(summarization_file)\n",
        "\n",
        "if not os.path.exists(log_root): os.makedirs(log_root)\n"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n",
            "/content/finished_files/chunked/train_*\n",
            "/content/drive/My Drive/code/get_to_the_point/finished_files/vocab\n",
            "/content/log\n",
            "data0_train\n",
            "drive/My Drive/code/get_to_the_point/pointer-generator-master/run_summarization.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rXxM2XNE_pjj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 复制 train 文件到虚拟机"
      ]
    },
    {
      "metadata": {
        "id": "9WQ3-CFgSDwk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# shutil.rmtree(exp_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ErldKfC3_pL1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if not os.path.exists(exp_dir):\n",
        "    if os.path.exists(drive_exp_dir):\n",
        "        shutil.copytree(drive_exp_dir, exp_dir)\n",
        "    else:\n",
        "        os.makedirs(exp_dir)\n",
        "        shutil.copytree(os.path.join(drive_log_root, \"train\"), os.path.join(exp_dir, \"train\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ampPDeJRepHs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 产生可执行的命令，复制在下一个 cell 执行即可"
      ]
    },
    {
      "metadata": {
        "id": "jDBmxPaVnxhN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "fe019108-0ed6-4277-ee65-65b658cfdd4d"
      },
      "cell_type": "code",
      "source": [
        "def get_command():\n",
        "    if(mode != \"decode\"):\n",
        "        sp = \"False\"\n",
        "    else:\n",
        "        sp = single_pass\n",
        "        \n",
        "    print(\"!python3 %s --mode=%s --data_path=%s --vocab_path=%s --log_root=%s --exp_name=%s --single_pass=%s --batch_size=%s\" \n",
        "                  % (summarization_file.replace(' ', '\\ '), mode, data_path, vocab_path.replace(' ', '\\ '), log_root, exp_name, sp, batch_size))\n",
        "\n",
        "get_command()"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "!python3 drive/My\\ Drive/code/get_to_the_point/pointer-generator-master/run_summarization.py --mode=train --data_path=/content/finished_files/chunked/train_* --vocab_path=/content/drive/My\\ Drive/code/get_to_the_point/finished_files/vocab --log_root=/content/log --exp_name=data0_train --single_pass=False --batch_size=256\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iuhcvwhyXa9v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 11798
        },
        "outputId": "de0ab900-72ca-45f5-a24c-650b42f50ef1"
      },
      "cell_type": "code",
      "source": [
        "# shell_command(\"python3 %s --mode=%s --data_path=%s --vocab_path=%s --log_root=%s --exp_name=%s --single_pass=%s\" \n",
        "#               % (summarization_file.replace(' ', '\\ '), mode, data_path, vocab_path.replace(' ', '\\ '), log_root, exp_name, single_pass))\n",
        "!python3 drive/My\\ Drive/code/get_to_the_point/pointer-generator-master/run_summarization.py --mode=train --data_path=/content/finished_files/chunked/train_* --vocab_path=/content/drive/My\\ Drive/code/get_to_the_point/finished_files/vocab --log_root=/content/log --exp_name=data0_train --single_pass=False --batch_size=256"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Starting seq2seq_attention in train mode...\n",
            "Warning: incorrectly formatted line in vocabulary file: 0800 555 111 356\n",
            "\n",
            "\n",
            "Warning: incorrectly formatted line in vocabulary file: 1800 333 000 139\n",
            "\n",
            "\n",
            "Warning: incorrectly formatted line in vocabulary file: 2 1/2 124\n",
            "\n",
            "\n",
            "Warning: incorrectly formatted line in vocabulary file: 3 1/2 86\n",
            "\n",
            "\n",
            "max_size of vocab was specified as 50000; we now have 50000 words. Stopping reading.\n",
            "Finished constructing vocabulary of 50000 total words. Last word added: cardholders\n",
            "HParams(mode='train', hidden_dim=256, emb_dim=128, batch_size=256, max_enc_steps=400, max_dec_steps=15, lr=0.15, adagrad_init_acc=0.1, rand_unif_init_mag=0.02, trunc_norm_init_std=0.0001, max_grad_norm=2.0, pointer_gen=True, coverage=False, cov_loss_wt=1.0)\n",
            "creating model...\n",
            "INFO:tensorflow:Building graph...\n",
            "Writing word embedding metadata file to /content/log/data0_train/train/vocab_metadata.tsv...\n",
            "INFO:tensorflow:Adding attention_decoder timestep 0 of 15\n",
            "INFO:tensorflow:Adding attention_decoder timestep 1 of 15\n",
            "INFO:tensorflow:Adding attention_decoder timestep 2 of 15\n",
            "INFO:tensorflow:Adding attention_decoder timestep 3 of 15\n",
            "INFO:tensorflow:Adding attention_decoder timestep 4 of 15\n",
            "INFO:tensorflow:Adding attention_decoder timestep 5 of 15\n",
            "INFO:tensorflow:Adding attention_decoder timestep 6 of 15\n",
            "INFO:tensorflow:Adding attention_decoder timestep 7 of 15\n",
            "INFO:tensorflow:Adding attention_decoder timestep 8 of 15\n",
            "INFO:tensorflow:Adding attention_decoder timestep 9 of 15\n",
            "INFO:tensorflow:Adding attention_decoder timestep 10 of 15\n",
            "INFO:tensorflow:Adding attention_decoder timestep 11 of 15\n",
            "INFO:tensorflow:Adding attention_decoder timestep 12 of 15\n",
            "INFO:tensorflow:Adding attention_decoder timestep 13 of 15\n",
            "INFO:tensorflow:Adding attention_decoder timestep 14 of 15\n",
            "INFO:tensorflow:Time to build graph: 67 seconds\n",
            "WARNING:tensorflow:From drive/My Drive/code/get_to_the_point/pointer-generator-master/run_summarization.py:172: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.MonitoredTrainingSession\n",
            "INFO:tensorflow:Preparing or waiting for session...\n",
            "2018-11-15 02:56:18.504784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2018-11-15 02:56:18.505524: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 11.17GiB freeMemory: 11.10GiB\n",
            "2018-11-15 02:56:18.505577: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\n",
            "2018-11-15 02:56:19.262525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2018-11-15 02:56:19.262604: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 \n",
            "2018-11-15 02:56:19.262633: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N \n",
            "2018-11-15 02:56:19.263037: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10758 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "INFO:tensorflow:Restoring parameters from /content/log/data0_train/train/model.ckpt-245289\n",
            "2018-11-15 02:56:19.705035: W tensorflow/core/framework/allocator.cc:122] Allocation of 25600000 exceeds 10% of system memory.\n",
            "2018-11-15 02:56:19.745381: W tensorflow/core/framework/allocator.cc:122] Allocation of 25600000 exceeds 10% of system memory.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Starting standard services.\n",
            "INFO:tensorflow:Saving checkpoint to path /content/log/data0_train/train/model.ckpt\n",
            "INFO:tensorflow:global_step/sec: 0\n",
            "INFO:tensorflow:Starting queue runners.\n",
            "INFO:tensorflow:Created session.\n",
            "INFO:tensorflow:starting run_training\n",
            "WARNING:tensorflow:Bucket input queue is empty when calling next_batch. Bucket queue size: 0, Input queue size: 18\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 11.601\n",
            "INFO:tensorflow:loss: 3.080125\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 4.150\n",
            "INFO:tensorflow:loss: 3.150471\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 4.134\n",
            "INFO:tensorflow:loss: 2.863352\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.447\n",
            "INFO:tensorflow:loss: 2.927858\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 4.174\n",
            "INFO:tensorflow:loss: 3.122259\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 4.102\n",
            "INFO:tensorflow:loss: 3.187431\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.631\n",
            "INFO:tensorflow:loss: 3.334561\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.953\n",
            "INFO:tensorflow:loss: 3.059232\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.003\n",
            "INFO:tensorflow:loss: 3.239427\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.975\n",
            "INFO:tensorflow:loss: 3.208070\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:global_step/sec: 0.166674\n",
            "INFO:tensorflow:Saving checkpoint to path /content/log/data0_train/train/model.ckpt\n",
            "INFO:tensorflow:seconds for training step: 3.550\n",
            "INFO:tensorflow:loss: 3.017834\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.190\n",
            "INFO:tensorflow:loss: 3.423368\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.012\n",
            "INFO:tensorflow:loss: 3.199117\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.783\n",
            "INFO:tensorflow:loss: 2.972816\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.129\n",
            "INFO:tensorflow:loss: 2.928636\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.935\n",
            "INFO:tensorflow:loss: 3.159337\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.958\n",
            "INFO:tensorflow:loss: 2.836853\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.005\n",
            "INFO:tensorflow:loss: 3.021418\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.032\n",
            "INFO:tensorflow:loss: 3.143102\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.157\n",
            "INFO:tensorflow:loss: 3.097054\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.977\n",
            "INFO:tensorflow:loss: 3.055244\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.979\n",
            "INFO:tensorflow:loss: 3.159159\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.362\n",
            "INFO:tensorflow:loss: 2.893770\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.264\n",
            "INFO:tensorflow:loss: 2.809376\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.396\n",
            "INFO:tensorflow:loss: 3.067204\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.084\n",
            "INFO:tensorflow:loss: 3.336390\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.238\n",
            "INFO:tensorflow:loss: 2.898023\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.173\n",
            "INFO:tensorflow:loss: 3.251996\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.076\n",
            "INFO:tensorflow:loss: 2.991002\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.940\n",
            "INFO:tensorflow:loss: 3.129235\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:global_step/sec: 0.333334\n",
            "INFO:tensorflow:Saving checkpoint to path /content/log/data0_train/train/model.ckpt\n",
            "INFO:tensorflow:seconds for training step: 3.150\n",
            "INFO:tensorflow:loss: 3.134353\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.406\n",
            "INFO:tensorflow:loss: 3.134875\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.943\n",
            "INFO:tensorflow:loss: 2.926696\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.934\n",
            "INFO:tensorflow:loss: 3.126398\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.951\n",
            "INFO:tensorflow:loss: 3.008442\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.960\n",
            "INFO:tensorflow:loss: 3.149620\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.272\n",
            "INFO:tensorflow:loss: 2.880773\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.901\n",
            "INFO:tensorflow:loss: 2.724663\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.961\n",
            "INFO:tensorflow:loss: 3.137469\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.946\n",
            "INFO:tensorflow:loss: 3.173738\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.271\n",
            "INFO:tensorflow:loss: 3.058892\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.094\n",
            "INFO:tensorflow:loss: 3.195539\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.296\n",
            "INFO:tensorflow:loss: 3.033505\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.937\n",
            "INFO:tensorflow:loss: 2.936520\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.966\n",
            "INFO:tensorflow:loss: 2.901653\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.599\n",
            "INFO:tensorflow:loss: 2.954302\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.976\n",
            "INFO:tensorflow:loss: 3.051373\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.965\n",
            "INFO:tensorflow:loss: 3.311748\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.970\n",
            "INFO:tensorflow:loss: 3.076602\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.985\n",
            "INFO:tensorflow:loss: 3.015260\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.979\n",
            "INFO:tensorflow:loss: 3.251724\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:global_step/sec: 0.349999\n",
            "INFO:tensorflow:Saving checkpoint to path /content/log/data0_train/train/model.ckpt\n",
            "INFO:tensorflow:seconds for training step: 3.329\n",
            "INFO:tensorflow:loss: 3.399699\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.983\n",
            "INFO:tensorflow:loss: 2.880328\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.982\n",
            "INFO:tensorflow:loss: 3.127920\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.017\n",
            "INFO:tensorflow:loss: 3.108189\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.859\n",
            "INFO:tensorflow:loss: 2.954713\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.056\n",
            "INFO:tensorflow:loss: 3.162796\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.964\n",
            "INFO:tensorflow:loss: 3.080229\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.946\n",
            "INFO:tensorflow:loss: 3.287768\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.935\n",
            "INFO:tensorflow:loss: 3.089468\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.963\n",
            "INFO:tensorflow:loss: 3.080411\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.986\n",
            "INFO:tensorflow:loss: 3.087660\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.062\n",
            "INFO:tensorflow:loss: 3.165647\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.006\n",
            "INFO:tensorflow:loss: 2.902347\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.041\n",
            "INFO:tensorflow:loss: 3.153247\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.139\n",
            "INFO:tensorflow:loss: 2.908432\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.069\n",
            "INFO:tensorflow:loss: 3.129274\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.128\n",
            "INFO:tensorflow:loss: 2.950880\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.497\n",
            "INFO:tensorflow:loss: 2.970093\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.056\n",
            "INFO:tensorflow:loss: 3.210558\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:global_step/sec: 0.316666\n",
            "INFO:tensorflow:Saving checkpoint to path /content/log/data0_train/train/model.ckpt\n",
            "INFO:tensorflow:seconds for training step: 3.180\n",
            "INFO:tensorflow:loss: 3.249999\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.744\n",
            "INFO:tensorflow:loss: 3.314519\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.088\n",
            "INFO:tensorflow:loss: 3.046468\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.185\n",
            "INFO:tensorflow:loss: 3.109276\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.016\n",
            "INFO:tensorflow:loss: 3.337357\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.090\n",
            "INFO:tensorflow:loss: 2.873938\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.365\n",
            "INFO:tensorflow:loss: 2.897604\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.136\n",
            "INFO:tensorflow:loss: 2.947562\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.626\n",
            "INFO:tensorflow:loss: 2.940247\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.435\n",
            "INFO:tensorflow:loss: 3.018971\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.152\n",
            "INFO:tensorflow:loss: 2.926404\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.336\n",
            "INFO:tensorflow:loss: 2.972408\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.317\n",
            "INFO:tensorflow:loss: 3.151381\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.117\n",
            "INFO:tensorflow:loss: 3.026098\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.193\n",
            "INFO:tensorflow:loss: 2.830086\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.127\n",
            "INFO:tensorflow:loss: 3.321331\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.151\n",
            "INFO:tensorflow:loss: 2.882333\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.153\n",
            "INFO:tensorflow:loss: 3.135133\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.790\n",
            "INFO:tensorflow:loss: 3.051124\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.581\n",
            "INFO:tensorflow:loss: 3.052589\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.042\n",
            "INFO:tensorflow:loss: 3.201934\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:global_step/sec: 0.350002\n",
            "INFO:tensorflow:Saving checkpoint to path /content/log/data0_train/train/model.ckpt\n",
            "INFO:tensorflow:seconds for training step: 3.181\n",
            "INFO:tensorflow:loss: 2.944323\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.055\n",
            "INFO:tensorflow:loss: 3.080498\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.980\n",
            "INFO:tensorflow:loss: 3.364148\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.733\n",
            "INFO:tensorflow:loss: 3.063379\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.236\n",
            "INFO:tensorflow:loss: 3.089333\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.113\n",
            "INFO:tensorflow:loss: 3.215279\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.071\n",
            "INFO:tensorflow:loss: 3.012241\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.109\n",
            "INFO:tensorflow:loss: 3.079015\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.024\n",
            "INFO:tensorflow:loss: 3.102789\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.106\n",
            "INFO:tensorflow:loss: 3.181158\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.086\n",
            "INFO:tensorflow:loss: 3.085675\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.076\n",
            "INFO:tensorflow:loss: 3.171656\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.182\n",
            "INFO:tensorflow:loss: 3.196498\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.312\n",
            "INFO:tensorflow:loss: 3.059705\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.527\n",
            "INFO:tensorflow:loss: 3.041215\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.125\n",
            "INFO:tensorflow:loss: 3.143126\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.137\n",
            "INFO:tensorflow:loss: 3.062756\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.339\n",
            "INFO:tensorflow:loss: 3.055812\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 1.996\n",
            "INFO:tensorflow:loss: 2.956645\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.027\n",
            "INFO:tensorflow:loss: 3.165870\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.308\n",
            "INFO:tensorflow:loss: 2.860894\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:global_step/sec: 0.349997\n",
            "INFO:tensorflow:Saving checkpoint to path /content/log/data0_train/train/model.ckpt\n",
            "INFO:tensorflow:seconds for training step: 2.857\n",
            "INFO:tensorflow:loss: 3.022205\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.065\n",
            "INFO:tensorflow:loss: 2.990709\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.079\n",
            "INFO:tensorflow:loss: 3.042055\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.119\n",
            "INFO:tensorflow:loss: 3.034996\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.212\n",
            "INFO:tensorflow:loss: 3.088430\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.105\n",
            "INFO:tensorflow:loss: 3.061466\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.039\n",
            "INFO:tensorflow:loss: 3.404914\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.119\n",
            "INFO:tensorflow:loss: 3.049984\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.088\n",
            "INFO:tensorflow:loss: 3.106689\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.984\n",
            "INFO:tensorflow:loss: 2.969791\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.382\n",
            "INFO:tensorflow:loss: 2.798136\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.991\n",
            "INFO:tensorflow:loss: 3.195597\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.983\n",
            "INFO:tensorflow:loss: 3.295470\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.008\n",
            "INFO:tensorflow:loss: 3.036727\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.015\n",
            "INFO:tensorflow:loss: 3.082076\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.070\n",
            "INFO:tensorflow:loss: 3.329266\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.020\n",
            "INFO:tensorflow:loss: 3.125833\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.054\n",
            "INFO:tensorflow:loss: 3.161667\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.311\n",
            "INFO:tensorflow:loss: 2.969766\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.148\n",
            "INFO:tensorflow:loss: 3.186780\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:global_step/sec: 0.333336\n",
            "INFO:tensorflow:Saving checkpoint to path /content/log/data0_train/train/model.ckpt\n",
            "INFO:tensorflow:seconds for training step: 3.235\n",
            "INFO:tensorflow:loss: 2.898055\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.075\n",
            "INFO:tensorflow:loss: 3.209342\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.094\n",
            "INFO:tensorflow:loss: 3.186856\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.045\n",
            "INFO:tensorflow:loss: 3.050725\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.007\n",
            "INFO:tensorflow:loss: 3.333414\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.026\n",
            "INFO:tensorflow:loss: 3.217190\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.161\n",
            "INFO:tensorflow:loss: 3.066292\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.022\n",
            "INFO:tensorflow:loss: 3.147220\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.174\n",
            "INFO:tensorflow:loss: 3.142553\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.200\n",
            "INFO:tensorflow:loss: 2.961194\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.035\n",
            "INFO:tensorflow:loss: 3.058602\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.123\n",
            "INFO:tensorflow:loss: 2.966417\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.128\n",
            "INFO:tensorflow:loss: 3.179044\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.164\n",
            "INFO:tensorflow:loss: 3.296206\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.148\n",
            "INFO:tensorflow:loss: 3.142783\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.069\n",
            "INFO:tensorflow:loss: 3.077478\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.033\n",
            "INFO:tensorflow:loss: 3.107425\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.525\n",
            "INFO:tensorflow:loss: 2.968027\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.258\n",
            "INFO:tensorflow:loss: 2.999854\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.074\n",
            "INFO:tensorflow:loss: 3.049838\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:global_step/sec: 0.333332\n",
            "INFO:tensorflow:Saving checkpoint to path /content/log/data0_train/train/model.ckpt\n",
            "INFO:tensorflow:seconds for training step: 3.334\n",
            "INFO:tensorflow:loss: 3.071499\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.581\n",
            "INFO:tensorflow:loss: 2.783932\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.401\n",
            "INFO:tensorflow:loss: 3.152355\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.711\n",
            "INFO:tensorflow:loss: 3.210707\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.420\n",
            "INFO:tensorflow:loss: 3.126970\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.194\n",
            "INFO:tensorflow:loss: 3.111632\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.488\n",
            "INFO:tensorflow:loss: 2.798237\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.142\n",
            "INFO:tensorflow:loss: 3.079143\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.103\n",
            "INFO:tensorflow:loss: 2.983672\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.082\n",
            "INFO:tensorflow:loss: 2.877042\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.211\n",
            "INFO:tensorflow:loss: 2.868197\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.105\n",
            "INFO:tensorflow:loss: 3.130928\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.103\n",
            "INFO:tensorflow:loss: 2.948761\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.056\n",
            "INFO:tensorflow:loss: 3.172658\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.100\n",
            "INFO:tensorflow:loss: 3.057300\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.064\n",
            "INFO:tensorflow:loss: 3.162004\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.068\n",
            "INFO:tensorflow:loss: 3.126439\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.152\n",
            "INFO:tensorflow:loss: 2.948109\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.187\n",
            "INFO:tensorflow:loss: 2.890967\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:global_step/sec: 0.316668\n",
            "INFO:tensorflow:Saving checkpoint to path /content/log/data0_train/train/model.ckpt\n",
            "INFO:tensorflow:seconds for training step: 3.389\n",
            "INFO:tensorflow:loss: 3.126401\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.143\n",
            "INFO:tensorflow:loss: 3.153559\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.075\n",
            "INFO:tensorflow:loss: 3.175343\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.130\n",
            "INFO:tensorflow:loss: 2.982615\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.185\n",
            "INFO:tensorflow:loss: 3.224278\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.578\n",
            "INFO:tensorflow:loss: 2.936021\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.059\n",
            "INFO:tensorflow:loss: 3.100756\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.976\n",
            "INFO:tensorflow:loss: 3.064329\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.985\n",
            "INFO:tensorflow:loss: 2.992544\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.163\n",
            "INFO:tensorflow:loss: 2.968687\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.542\n",
            "INFO:tensorflow:loss: 2.830712\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.998\n",
            "INFO:tensorflow:loss: 3.256575\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.010\n",
            "INFO:tensorflow:loss: 3.311753\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.945\n",
            "INFO:tensorflow:loss: 3.247613\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.972\n",
            "INFO:tensorflow:loss: 3.232295\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.979\n",
            "INFO:tensorflow:loss: 3.152029\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.964\n",
            "INFO:tensorflow:loss: 3.000587\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.992\n",
            "INFO:tensorflow:loss: 3.071912\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.291\n",
            "INFO:tensorflow:loss: 3.014462\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 1.870\n",
            "INFO:tensorflow:loss: 3.168797\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.980\n",
            "INFO:tensorflow:loss: 3.019789\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:Saving checkpoint to path /content/log/data0_train/train/model.ckpt\n",
            "INFO:tensorflow:seconds for training step: 3.295\n",
            "INFO:tensorflow:loss: 3.196750\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.296\n",
            "INFO:tensorflow:loss: 2.970417\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.958\n",
            "INFO:tensorflow:loss: 3.080219\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 1.970\n",
            "INFO:tensorflow:loss: 3.029800\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 3.035\n",
            "INFO:tensorflow:loss: 3.035274\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.949\n",
            "INFO:tensorflow:loss: 3.283360\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.980\n",
            "INFO:tensorflow:loss: 2.923464\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.957\n",
            "INFO:tensorflow:loss: 2.781232\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.968\n",
            "INFO:tensorflow:loss: 3.139565\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.974\n",
            "INFO:tensorflow:loss: 3.154700\n",
            "INFO:tensorflow:running training step...\n",
            "INFO:tensorflow:seconds for training step: 2.982\n",
            "INFO:tensorflow:loss: 3.055183\n",
            "INFO:tensorflow:running training step...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EmDbkRm5RA2X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zaapHmoCf7rA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 解码测试"
      ]
    },
    {
      "metadata": {
        "id": "LGetveVOf-fy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mode = \"decode\"\n",
        "data_path = \"/content/finished_files/chunked/test_*\"\n",
        "single_pass = \"True\"\n",
        "\n",
        "get_command()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "27L9PpJCAQcl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 复制 log 文件到云盘"
      ]
    },
    {
      "metadata": {
        "id": "9EC2_-1jv5Wm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "60a34138-0e52-4dfb-ca83-bc9ffe077daa"
      },
      "cell_type": "code",
      "source": [
        "if os.path.exists(drive_exp_dir): \n",
        "    shutil.rmtree(drive_exp_dir)\n",
        "\n",
        "shutil.copytree(exp_dir, drive_exp_dir)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/code/get_to_the_point/log/data0_train'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "metadata": {
        "id": "x0Tb8t3KAuVE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}